# Module: audit
# Rules for generating audit summaries (summary.json)
# Implementation: Story 2.2b
#
# This module generates summary.json files for standardized FASTA files:
#   results/standardized/{species}/
#   ├── genome.fa.gz
#   ├── genome.summary.json           # Generated by this module
#   ├── proteins.longest.fa.gz
#   └── proteins.longest.summary.json # Generated by this module

from pathlib import Path


# =============================================================================
# Summary Generation Rules
# =============================================================================

rule generate_fasta_summary:
    """
    Generate summary.json for a single FASTA file.

    Produces statistics including:
    - sequence_count, total_length, mean/median/min/max length
    - n50, gc_content (nucleotide), n_content
    - length_histogram
    - id_format detection

    AC4: Audit summary generation (summary.json)
    """
    input:
        fasta="{output_dir}/standardized/{species}/{file}.fa.gz"
    output:
        summary="{output_dir}/standardized/{species}/{file}.summary.json"
    log:
        "{output_dir}/logs/generate_fasta_summary/{species}/{file}.log"
    run:
        import logging
        from pathlib import Path

        # Set up file handler for this rule
        logger = logging.getLogger(f"generate_fasta_summary.{wildcards.species}.{wildcards.file}")
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler(log[0])
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)

        try:
            from workflow.lib.bio_utils import write_summary_json

            input_path = Path(input.fasta)
            output_path = Path(output.summary)

            logger.info(f"Generating summary for: {input_path}")

            result_path = write_summary_json(input_path, output_path)

            logger.info(f"Summary written to: {result_path}")
        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            raise
        finally:
            handler.close()
            logger.removeHandler(handler)


# Note: generate_genome_summary and generate_proteins_summary are handled by
# the generic generate_fasta_summary rule via wildcard matching.
# Removed redundant specialized rules per DRY principle (code review 2026-01-29).


# =============================================================================
# Aggregate Rules
# =============================================================================

rule generate_all_summaries:
    """
    Generate summary.json files for all standardized FASTA files.

    This rule triggers summary generation for:
    - All genome files
    - All protein files
    for all local species in the configuration.

    AC4: Batch summary generation
    """
    input:
        genome_summaries=expand(
            "{output_dir}/standardized/{species}/genome.summary.json",
            output_dir=get_output_dir(),
            species=get_local_species()
        ),
        protein_summaries=expand(
            "{output_dir}/standardized/{species}/proteins.longest.summary.json",
            output_dir=get_output_dir(),
            species=get_local_species()
        )
    output:
        touch("{output_dir}/standardized/.summaries_complete")
    params:
        output_dir=get_output_dir()
    log:
        "{output_dir}/logs/generate_all_summaries.log"
    run:
        import logging

        logger = logging.getLogger("generate_all_summaries")
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler(log[0])
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)

        try:
            genome_count = len(input.genome_summaries)
            protein_count = len(input.protein_summaries)

            logger.info(f"All summaries generated:")
            logger.info(f"  Genome summaries: {genome_count}")
            logger.info(f"  Protein summaries: {protein_count}")
            logger.info(f"  Total: {genome_count + protein_count}")
        finally:
            handler.close()
            logger.removeHandler(handler)


# =============================================================================
# Consistency Check Rules
# =============================================================================

rule check_id_consistency:
    """
    Check ID consistency between input FASTA and tool outputs.

    Generates a consistency report comparing:
    - Input protein FASTA IDs
    - OrthoFinder output IDs (when available)
    - eggNOG-mapper output IDs (when available)

    AC3: ID consistency checking
    """
    input:
        proteins="{output_dir}/standardized/{species}/proteins.longest.fa.gz"
    output:
        report="{output_dir}/standardized/{species}/id_consistency.json"
    log:
        "{output_dir}/logs/check_id_consistency/{species}.log"
    run:
        import json
        import logging
        from datetime import datetime, timezone
        from pathlib import Path

        logger = logging.getLogger(f"check_id_consistency.{wildcards.species}")
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler(log[0])
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)

        try:
            from workflow.lib.bio_utils import extract_ids, generate_audit_summary

            proteins_path = Path(input.proteins)

            logger.info(f"Checking ID consistency for: {wildcards.species}")

            # Extract IDs from protein FASTA
            ids = extract_ids(proteins_path)
            summary = generate_audit_summary(proteins_path)

            report = {
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "species": wildcards.species,
                "input_file": str(proteins_path),
                "id_count": len(ids),
                "id_format": summary.get("id_format", {}),
                "sample_ids": sorted(list(ids))[:10],
                "status": "OK",
                "notes": "Baseline ID extraction complete. Consistency with downstream tools will be checked after their execution."
            }

            output_path = Path(output.report)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            logger.info(f"ID consistency baseline report written: {output_path}")
            logger.info(f"  Total IDs: {len(ids)}")
        except Exception as e:
            logger.error(f"ID consistency check failed: {e}")
            raise
        finally:
            handler.close()
            logger.removeHandler(handler)
